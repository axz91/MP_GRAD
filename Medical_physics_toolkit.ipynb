{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axz91/mp/blob/main/Medical_physics_toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/sample1.zip /content/sample1\n"
      ],
      "metadata": {
        "id": "0DPUTi5ywLA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw"
      ],
      "metadata": {
        "id": "lZZhsWk-d4GC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd59cdc6-b114-46e4-eb44-17a8e713f6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/191.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.6.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.3.0 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvRAbf7-Xk9L",
        "outputId": "2ab041f6-873c-4f10-b05d-3c80316643d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from weasyprint import HTML\n",
        "\n",
        "dir_path = \"/content/drive/MyDrive/Takeout/\"\n",
        "\n",
        "# Get a list of HTML files\n",
        "html_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.html')]\n",
        "\n",
        "# Combine all HTML files into a single HTML file\n",
        "combined_html = \"\"\n",
        "for file in html_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "        combined_html += str(soup)\n",
        "\n",
        "# Write combined HTML to a single file\n",
        "combined_file = os.path.join(dir_path, \"combined.html\")\n",
        "with open(combined_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(combined_html)\n",
        "\n",
        "# Convert combined HTML to PDF\n",
        "HTML(combined_file).write_pdf(os.path.join(dir_path, \"output.pdf\"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D8FEBOgEd4K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"NwWQOcQJd4KLi3vg1-8Sdg\",\n",
        "    client_secret=\"L6Gsl522l-8F4r_wf0C7zuQqaNA52w\",\n",
        "    user_agent=\"mp\"\n",
        ")\n",
        "\n",
        "subreddit = reddit.subreddit(\"MedicalPhysics\")\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists('/content/drive/MyDrive/Takeout'):\n",
        "    os.makedirs('/content/drive/MyDrive/Takeout')\n",
        "\n",
        "def write_comments(comments, f, level=0):\n",
        "    # Sort comments by creation time (newest first)\n",
        "    sorted_comments = sorted(comments, key=lambda comment: comment.created_utc if isinstance(comment, praw.models.Comment) else 0, reverse=True)\n",
        "\n",
        "    for comment in sorted_comments:\n",
        "        if isinstance(comment, praw.models.Comment):  # Check if comment is an actual comment\n",
        "            # If it's a top-level comment, make it bold\n",
        "            if level == 0:\n",
        "                f.write(\">\" * level + \"<p><b>\" + comment.body + \"</b></p>\\n\")  # Write comment body in bold\n",
        "            else:\n",
        "                f.write(\">\" * level + \"<p>\" + comment.body + \"</p>\\n\")  # Write comment body\n",
        "            if len(comment.replies) > 0:\n",
        "                write_comments(comment.replies, f, level+1)  # Recursively handle replies\n",
        "        elif isinstance(comment, praw.models.MoreComments):\n",
        "            more_comments = comment.comments()\n",
        "            write_comments(more_comments, f, level)  # Recursively handle MoreComments\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Search for posts with \"Career Question\" in the title\n",
        "for post in subreddit.search(\"Career Question\"):\n",
        "    post.comments.replace_more(limit=None)  # Fetch all comments\n",
        "\n",
        "\n",
        "    # Create a timestamp string for the post creation time\n",
        "    timestamp = datetime.datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    filename = f\"/content/drive/MyDrive/Takeout/{timestamp}_{post.id}.html\"\n",
        "\n",
        "        # Check if file already exists\n",
        "    if not os.path.isfile(filename):\n",
        "            with open(filename, \"w\") as f:\n",
        "                f.write(\"<h3>Post Title: \" + post.title + \"</h3>\\n\")\n",
        "                f.write(\"<p>Post Content: \" + post.selftext + \"</p>\\n\")\n",
        "                f.write(\"<h3>Comments:</h3>\\n\")\n",
        "                write_comments(post.comments, f)\n",
        "                f.write(\"<p>Original URL: <a href='\" + post.url + \"'>\" + post.url + \"</a></p>\\n\")  # Add the original URL as a clickable link\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zitnLEGTByaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import os\n",
        "import datetime\n",
        "import asyncpraw\n",
        "\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=\"NwWQOcQJd4KLi3vg1-8Sdg\",\n",
        "    client_secret=\"L6Gsl522l-8F4r_wf0C7zuQqaNA52w\",\n",
        "    user_agent=\"mp\"\n",
        ")\n",
        "\n",
        "subreddit = reddit.subreddit(\"MedicalPhysics\")\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists('/content/drive/MyDrive/Takeout/'):\n",
        "    os.makedirs('/content/drive/MyDrive/Takeout/')\n",
        "\n",
        "\n",
        "\n",
        "import praw\n",
        "import praw.models\n",
        "\n",
        "\n",
        "def write_comments(comments, f, level=0):\n",
        "    # Sort comments by creation time (newest first)\n",
        "    sorted_comments = sorted(comments, key=lambda comment: comment.created_utc if isinstance(comment, praw.models.Comment) else 0, reverse=True)\n",
        "\n",
        "    for comment in sorted_comments:\n",
        "        if isinstance(comment, praw.models.Comment):  # Check if comment is an actual comment\n",
        "            # If it's a top-level comment, make it bold\n",
        "            if level == 0:\n",
        "                f.write(\">\" * level + \"<p><b>\" + comment.body + \"</b></p>\\n\")  # Write comment body in bold\n",
        "            else:\n",
        "                f.write(\">\" * level + \"<p>\" + comment.body + \"</p>\\n\")  # Write comment body\n",
        "            if len(comment.replies) > 0:\n",
        "                write_comments(comment.replies, f, level+1)  # Recursively handle replies\n",
        "        elif isinstance(comment, praw.models.MoreComments):\n",
        "            more_comments = comment.comments()\n",
        "            write_comments(more_comments, f, level)  # Recursively handle MoreComments\n",
        "\n",
        "\n",
        "# Define your time cutoff\n",
        "cutoff = datetime.datetime(2021, 12, 31)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define your time cutoff\n",
        "cutoff = datetime.datetime(2021, 12, 1)\n",
        "\n",
        "# Search for posts with \"Career Question\" in the title\n",
        "for post in subreddit.search(\"Career Question\"):\n",
        "    post_date = datetime.datetime.fromtimestamp(post.created_utc)\n",
        "    # Check if post is before the cutoff\n",
        "    if post_date < cutoff:\n",
        "        post.comments.replace_more(limit=None)  # Fetch all comments\n",
        "\n",
        "    # Create a timestamp string for the post creation time\n",
        "    timestamp = datetime.datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    filename = f\"/content/drive/MyDrive/Takeout/{timestamp}_{post.id}.html\"\n",
        "\n",
        "    # Check if file already exists\n",
        "    if not os.path.isfile(filename):\n",
        "        with open(filename, \"w\") as f:\n",
        "            f.write(\"<h3>Post Title: \" + post.title + \"</h3>\\n\")\n",
        "            f.write(\"<p>Post Content: \" + post.selftext + \"</p>\\n\")\n",
        "            f.write(\"<h3>Comments:</h3>\\n\")\n",
        "            write_comments(post.comments, f)\n",
        "            f.write(\"<p>Original URL: <a href='\" + post.url + \"'>\" + post.url + \"</a></p>\\n\")  # Add the original URL as a clickable link"
      ],
      "metadata": {
        "id": "b9t4XpRrGhn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# Directory path\n",
        "dir_path = \"/content/drive/MyDrive/Takeout/\"\n",
        "\n",
        "# Find all HTML files in the directory\n",
        "files = glob.glob(dir_path + \"*.html\")\n",
        "\n",
        "# Remove the directory path from each file name\n",
        "#files = [os.path.basename(file) for file in files]\n",
        "\n",
        "print(files)"
      ],
      "metadata": {
        "id": "Uh1QweISIByi",
        "outputId": "ffa7cfc3-dea2-47fc-9987-6fd4a6546c08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Takeout/2023-05-09_11-00-22_13cmb3x_cg.html', '/content/drive/MyDrive/Takeout/2023-06-06_11-00-21_142c99b_cg.html', '/content/drive/MyDrive/Takeout/2023-04-04_11-00-24_12bffof_cg.html', '/content/drive/MyDrive/Takeout/2023-03-28_11-00-22_124kzho_cg.html', '/content/drive/MyDrive/Takeout/2023-04-18_11-00-25_12qjhhq_cg.html', '/content/drive/MyDrive/Takeout/2023-03-07_12-00-17_11kxfd8_cg.html', '/content/drive/MyDrive/Takeout/2023-03-21_11-00-23_11xdiwh_cg.html', '/content/drive/MyDrive/Takeout/2023-02-21_12-00-19_1181sps_cg.html', '/content/drive/MyDrive/Takeout/2023-02-07_12-00-09_10w03e2_cg.html', '/content/drive/MyDrive/Takeout/2023-01-31_12-00-08_10px85n_cg.html', '/content/drive/MyDrive/Takeout/2023-01-24_12-00-10_10k3uuo_cg.html', '/content/drive/MyDrive/Takeout/2022-12-13_12-00-12_zku0bf_cg.html', '/content/drive/MyDrive/Takeout/2022-12-27_12-00-11_zwe2jz_cg.html', '/content/drive/MyDrive/Takeout/2023-01-03_12-00-12_1026bim_cg.html', '/content/drive/MyDrive/Takeout/2023-02-14_12-00-10_1123p0l_cg.html', '/content/drive/MyDrive/Takeout/2022-12-20_12-00-08_zqmkjz_cg.html', '/content/drive/MyDrive/Takeout/2023-01-10_12-00-08_1088418_cg.html', '/content/drive/MyDrive/Takeout/2023-01-17_12-00-11_10eajpi_cg.html', '/content/drive/MyDrive/Takeout/2022-11-29_12-00-08_z7s38k_cg.html', '/content/drive/MyDrive/Takeout/2022-11-22_12-00-11_z1rs59_cg.html', '/content/drive/MyDrive/Takeout/2022-11-15_12-00-12_yvv440_cg.html', '/content/drive/MyDrive/Takeout/2022-12-06_12-00-10_ze4s5i_cg.html', '/content/drive/MyDrive/Takeout/2022-11-01_11-00-11_yj5o0d_cg.html', '/content/drive/MyDrive/Takeout/2022-11-08_12-00-10_ypk1do_cg.html', '/content/drive/MyDrive/Takeout/2022-10-11_11-00-13_y1635c_cg.html', '/content/drive/MyDrive/Takeout/2022-10-18_11-00-12_y73vd5_cg.html', '/content/drive/MyDrive/Takeout/2022-09-13_11-00-12_xd4grj_cg.html', '/content/drive/MyDrive/Takeout/2022-10-25_11-00-09_yd1yax_cg.html', '/content/drive/MyDrive/Takeout/2022-09-27_11-00-12_xpdjeq_cg.html', '/content/drive/MyDrive/Takeout/2022-08-02_11-00-10_we8ogw_cg.html', '/content/drive/MyDrive/Takeout/2022-09-06_11-00-11_x77lsq_cg.html', '/content/drive/MyDrive/Takeout/2022-08-30_11-00-08_x1g20h_cg.html', '/content/drive/MyDrive/Takeout/2022-08-16_11-00-11_wprah9_cg.html', '/content/drive/MyDrive/Takeout/2022-08-23_11-00-12_wvlrxp_cg.html', '/content/drive/MyDrive/Takeout/2022-10-04_11-00-12_xvc15w_cg.html', '/content/drive/MyDrive/Takeout/2022-07-19_11-00-14_w2pxu4_cg.html', '/content/drive/MyDrive/Takeout/2022-08-05_20-05-16_wh4yfu_cg.html', '/content/drive/MyDrive/Takeout/2022-07-26_11-00-11_w8gt5f_cg.html', '/content/drive/MyDrive/Takeout/2022-07-12_11-00-11_vx8gxj_cg.html', '/content/drive/MyDrive/Takeout/2022-06-21_11-00-12_vhao4w_cg.html', '/content/drive/MyDrive/Takeout/2022-05-03_11-00-10_uhdi9e_cg.html', '/content/drive/MyDrive/Takeout/2022-05-10_11-00-09_umg1tu_cg.html', '/content/drive/MyDrive/Takeout/2022-06-14_11-00-11_vc0wci_cg.html', '/content/drive/MyDrive/Takeout/2022-07-05_11-00-10_vrv7ic_cg.html', '/content/drive/MyDrive/Takeout/2022-05-31_11-00-11_v1nmme_cg.html', '/content/drive/MyDrive/Takeout/2022-04-19_11-00-11_u72foa_cg.html', '/content/drive/MyDrive/Takeout/2022-03-15_11-00-10_temd8w_cg.html', '/content/drive/MyDrive/Takeout/2022-05-24_11-00-11_uwo5eu_cg.html', '/content/drive/MyDrive/Takeout/2022-04-26_11-00-11_uc9vpl_cg.html', '/content/drive/MyDrive/Takeout/2022-03-08_12-00-11_t9fa3k_cg.html', '/content/drive/MyDrive/Takeout/2022-06-28_11-00-13_vmkij3_cg.html', '/content/drive/MyDrive/Takeout/2022-02-08_12-00-15_snijga_cg.html', '/content/drive/MyDrive/Takeout/2022-01-04_12-00-14_rvss65_cg.html', '/content/drive/MyDrive/Takeout/2022-03-01_12-00-14_t46cbf_cg.html', '/content/drive/MyDrive/Takeout/2022-02-22_12-00-18_sylwl9_cg.html', '/content/drive/MyDrive/Takeout/2022-01-25_12-00-13_scc3uh_cg.html', '/content/drive/MyDrive/Takeout/2022-01-18_12-00-20_s6vx3f_cg.html', '/content/drive/MyDrive/Takeout/2022-05-17_11-00-11_urjuyv_cg.html', '/content/drive/MyDrive/Takeout/2022-01-11_12-00-14_s1bbbx_cg.html', '/content/drive/MyDrive/Takeout/2022-03-22_11-00-09_tk0cuu_cg.html', '/content/drive/MyDrive/Takeout/2022-06-07_11-00-11_v6sfzy_cg.html', '/content/drive/MyDrive/Takeout/2021-12-14_12-00-15_rg5zse_cg.html', '/content/drive/MyDrive/Takeout/2021-12-21_12-00-19_rld76f_cg.html', '/content/drive/MyDrive/Takeout/2021-11-30_12-00-24_r5lrls_cg.html', '/content/drive/MyDrive/Takeout/2021-11-16_12-00-15_qv69d0_cg.html', '/content/drive/MyDrive/Takeout/2021-10-05_11-00-22_q1u1g0_cg.html', '/content/drive/MyDrive/Takeout/2021-11-02_11-00-16_ql1p4j_cg.html', '/content/drive/MyDrive/Takeout/2021-11-09_12-00-14_qq25it_cg.html', '/content/drive/MyDrive/Takeout/2021-10-26_11-00-12_qg3vlx_cg.html', '/content/drive/MyDrive/Takeout/2021-11-23_12-00-11_r0bo0k_cg.html', '/content/drive/MyDrive/Takeout/2021-07-27_11-00-14_oskmxb_cg.html', '/content/drive/MyDrive/Takeout/2021-12-07_12-00-19_raxlyu_cg.html', '/content/drive/MyDrive/Takeout/2021-08-31_11-00-12_pf3jb6_cg.html', '/content/drive/MyDrive/Takeout/2021-08-17_11-00-15_p61ht9_cg.html', '/content/drive/MyDrive/Takeout/2021-02-23_12-00-19_lqgs64_cg.html', '/content/drive/MyDrive/Takeout/2021-06-01_11-00-23_nprb6s_cg.html', '/content/drive/MyDrive/Takeout/2021-07-13_11-00-11_ojd7z6_cg.html', '/content/drive/MyDrive/Takeout/2023-07-11_11-00-22_14wo2bk_cg.html', '/content/drive/MyDrive/Takeout/2023-07-04_11-00-23_14qc401_cg.html', '/content/drive/MyDrive/Takeout/2023-06-13_11-00-22_148cojx_cg.html', '/content/drive/MyDrive/Takeout/2023-06-20_11-00-33_14e80d2_cg.html', '/content/drive/MyDrive/Takeout/2023-06-27_11-00-22_14kaiwx_cg.html', '/content/drive/MyDrive/Takeout/2023-05-16_11-00-23_13j1y5n_cg.html', '/content/drive/MyDrive/Takeout/2023-05-02_11-00-21_135hrv5_cg.html', '/content/drive/MyDrive/Takeout/2023-04-11_11-00-25_12ig51p_cg.html', '/content/drive/MyDrive/Takeout/2023-03-14_11-00-20_11r33qn_cg.html', '/content/drive/MyDrive/Takeout/2022-09-20_11-00-11_xj5o48_cg.html', '/content/drive/MyDrive/Takeout/2022-04-05_11-00-10_twsaqe_cg.html', '/content/drive/MyDrive/Takeout/2022-02-01_12-00-19_shtbpk_cg.html', '/content/drive/MyDrive/Takeout/2021-12-28_12-00-14_rqe9xq_cg.html', '/content/drive/MyDrive/Takeout/index.html']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "dir_path = \"./\"\n",
        "\n",
        "\n",
        "\n",
        "# You should define here how to get the file list. For example:\n",
        "# for file in os.listdir(dir_path):\n",
        "#     if file.endswith(\".html\"):\n",
        "#         files.append(os.path.join(dir_path, file))\n",
        "\n",
        "# List to store titles\n",
        "titles = []\n",
        "\n",
        "\n",
        "titles = []\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        with open(file, 'r') as f:\n",
        "            soup = BeautifulSoup(f, 'html.parser')\n",
        "            title = soup.title.string if soup.title else None\n",
        "            if title:\n",
        "                titles.append(title)\n",
        "            else:\n",
        "                first_line = f.readline().strip()\n",
        "                titles.append(first_line)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {file}. Error: {str(e)}\")\n",
        "        titles.append('No Title')\n",
        "\n",
        "# Print the titles\n",
        "for title in titles:\n",
        "    print(title)\n",
        "\n",
        "\n",
        "# Get the base name for each file\n",
        "files_basename = [os.path.basename(file) for file in files]\n",
        "\n",
        "# Start of the HTML content\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Combined Page</title>\n",
        "    <style>\n",
        "        #nav {\n",
        "            width: 200px;\n",
        "            height: 1500px; /* set the height */\n",
        "            overflow-y: auto; /* add a vertical scrollbar */\n",
        "            float: left;\n",
        "            background-color: #f5f5f5; /* set a background color for better visibility */\n",
        "            padding: 10px; /* add some padding */\n",
        "            box-sizing: border-box; /* include padding and border in the element's total width and height */\n",
        "        }\n",
        "        #content {\n",
        "            margin-left: 210px;\n",
        "        }\n",
        "        iframe {\n",
        "            width: 100%;\n",
        "            height: 1500px;\n",
        "        }\n",
        "        ul {\n",
        "            list-style-type: none; /* remove bullets */\n",
        "            padding: 0; /* remove padding */\n",
        "        }\n",
        "        li a {\n",
        "            display: block; /* make the links block elements */\n",
        "            color: #000; /* set the link color */\n",
        "            padding: 8px 16px; /* add some padding */\n",
        "            text-decoration: none; /* remove underline */\n",
        "        }\n",
        "        li a:hover {\n",
        "            background-color: #555; /* change background color on hover */\n",
        "            color: white; /* change text color on hover */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div id=\"nav\">\n",
        "        <ul>\n",
        "\"\"\"\n",
        "\n",
        "# Add each file to the navigation bar\n",
        "for i, fname, title, basename in zip(range(len(files)), files, titles, files_basename):\n",
        "    html_content += '            <li><a href=\"{}\" target=\"content-frame\">{}</a></li>\\n'.format(basename, title)\n",
        "\n",
        "\n",
        "# End of the HTML content\n",
        "html_content += \"\"\"\n",
        "        </ul>\n",
        "    </div>\n",
        "    <div id=\"content\">\n",
        "        <iframe name=\"content-frame\" src=\"https://docs.google.com/document/d/e/2PACX-1vRCEUpALD_kARJB5D6Zq3npEACKPqi0p_qOwZTMHkNRBPrPfzO8d-itYZqFomwFIeu2yJ55d-WKftRR/pub?embedded=true\"></iframe>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\n",
        "\"\"\".format(files_basename[0])  # Load the first file by default\n",
        "\n",
        "# Write the HTML content to a new file\n",
        "with open(\"/content/drive/MyDrive/Takeout/index.html\", \"w\") as f:\n",
        "    f.write(html_content)"
      ],
      "metadata": {
        "id": "wsH5cYZFIB5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "dir_path = \"./\"\n",
        "\n",
        "\n",
        "\n",
        "# List to store titles\n",
        "titles = []\n",
        "\n",
        "# Parse each file and extract the title\n",
        "for file in files:\n",
        "    with open(file, 'r') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "        title = soup.title.string if soup.title else 'No Title'\n",
        "        titles.append(title)\n",
        "\n",
        "# Get the base name for each file\n",
        "files_basename = [os.path.basename(file) for file in files]\n",
        "\n",
        "# Start of the HTML content\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Combined Page</title>\n",
        "    <style>\n",
        "        #nav {\n",
        "            width: 200px;\n",
        "            height: 1500px; /* set the height */\n",
        "            overflow-y: auto; /* add a vertical scrollbar */\n",
        "            float: left;\n",
        "            background-color: #f5f5f5; /* set a background color for better visibility */\n",
        "            padding: 10px; /* add some padding */\n",
        "            box-sizing: border-box; /* include padding and border in the element's total width and height */\n",
        "        }\n",
        "        #content {\n",
        "            margin-left: 210px;\n",
        "        }\n",
        "        iframe {\n",
        "            width: 100%;\n",
        "            height: 1500px;\n",
        "        }\n",
        "        ul {\n",
        "            list-style-type: none; /* remove bullets */\n",
        "            padding: 0; /* remove padding */\n",
        "        }\n",
        "        li a {\n",
        "            display: block; /* make the links block elements */\n",
        "            color: #000; /* set the link color */\n",
        "            padding: 8px 16px; /* add some padding */\n",
        "            text-decoration: none; /* remove underline */\n",
        "        }\n",
        "        li a:hover {\n",
        "            background-color: #555; /* change background color on hover */\n",
        "            color: white; /* change text color on hover */\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div id=\"nav\">\n",
        "        <ul>\n",
        "\"\"\"\n",
        "\n",
        "# Add each file to the navigation bar\n",
        "for i, fname, title, basename in zip(range(len(files)), files, titles, files_basename):\n",
        "    html_content += '            <li><a href=\"{}\" target=\"content-frame\">{}</a></li>\\n'.format(basename, title)\n",
        "\n",
        "# End of the HTML content\n",
        "html_content += \"\"\"\n",
        "        </ul>\n",
        "    </div>\n",
        "    <div id=\"content\">\n",
        "        <iframe name=\"content-frame\" src=\"{}\"></iframe>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\"\"\".format(files_basename[0])  # Load the first file by default\n",
        "# Write the HTML content to a new file\n",
        "with open(\"/content/drive/MyDrive/Takeout/index.html\", \"w\") as f:\n",
        "    f.write(html_content)"
      ],
      "metadata": {
        "id": "rrCNiHG8B2kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 nltk wordcloud matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA2DH75F76Om",
        "outputId": "22b7e8db-af8a-48c9-e268-f25784af5804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# If running for the first time, you'll need to download the stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "dir_path = \"/content/drive/MyDrive/Takeout/\"\n",
        "\n",
        "# get the stopwords for English\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# dictionary to hold the frequency of each word\n",
        "word_freq = {}\n",
        "\n",
        "# extract text from each file and count the words\n",
        "for file in os.listdir(dir_path):\n",
        "    if file.endswith(\".html\"):\n",
        "        with open(os.path.join(dir_path, file), 'r') as f:\n",
        "            soup = BeautifulSoup(f, 'html.parser')\n",
        "            # get text from the HTML file\n",
        "            text = soup.get_text()\n",
        "            # tokenize the text\n",
        "            word_tokens = word_tokenize(text)\n",
        "            # remove punctuation and convert to lowercase\n",
        "            word_tokens = [word.lower() for word in word_tokens if word.isalpha()]\n",
        "            # remove stopwords\n",
        "            filtered_words = [word for word in word_tokens if word not in stop_words]\n",
        "            # count the words\n",
        "            for word in filtered_words:\n",
        "                if word not in word_freq:\n",
        "                    word_freq[word] = 1\n",
        "                else:\n",
        "                    word_freq[word] += 1\n",
        "\n",
        "# generate a word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "# display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "nIfSdAgu7p5W",
        "outputId": "eb6a088f-b4fc-48f1-e475-fce8adef7979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ba921ff2ad2e>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# extract text from each file and count the words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Takeout/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "bbEGhZR-KhAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5593ec-c1d6-46c0-b552-06b22b61b91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json"
      ],
      "metadata": {
        "id": "YjbPBGlwLTwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import openai\n",
        "openai.organization = \"org-OXdgUqNqxOm6OI1mItPsFCyi\"\n",
        "openai.api_key = \"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\"\n",
        "openai.Model.list()"
      ],
      "metadata": {
        "id": "8B64bwQMLT4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "directory = '/content/drive/MyDrive/Takeout/'\n",
        "\n",
        "# Retrieve all HTML files in the directory\n",
        "html_files = [file for file in os.listdir(directory) if file.endswith('.html')]\n",
        "\n",
        "# Specify the index or file name of the HTML file you want to read\n",
        "file_index = 0  # Change this to the desired index or specify the file name\n",
        "\n",
        "# Check if the index is within the range of the html_files list\n",
        "if 0 <= file_index < len(html_files):\n",
        "    file_name = html_files[file_index]\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    # Open the HTML file and read its contents\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract the full text from the HTML\n",
        "    full_text = soup.get_text()\n",
        "\n",
        "    # Print or process the full text as needed\n",
        "    print(full_text)\n",
        "else:\n",
        "    print(\"Invalid file index.\")"
      ],
      "metadata": {
        "id": "KYHQ0hWPLT70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install manifest-ml"
      ],
      "metadata": {
        "id": "adrrOQYPqBhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "conn = sqlite3.connect('database.db')\n",
        "cursor = conn.cursor()"
      ],
      "metadata": {
        "id": "FpBDz8QGs7Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "import html\n",
        "\n",
        "directory = '/content/drive/MyDrive/Takeout/'\n",
        "\n",
        "# Retrieve all HTML files in the directory\n",
        "html_files = [file for file in os.listdir(directory) if file.endswith('.html')]\n",
        "\n",
        "def process_html_file(file_path):\n",
        "    # Open the HTML file and read its contents\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract the full text from the HTML\n",
        "    full_text = soup.get_text()\n",
        "\n",
        "    # Assign the processed HTML content to the question variable\n",
        "    question = full_text\n",
        "\n",
        "    # Call the openai.ChatCompletion.create() function with the updated question\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=[\n",
        "    {\"role\":\"system\",\"content\": \"Your paraphrasing must not use first point of view like sentences 'I was', 'I aam'!!!!! \"},\n",
        "       {\"role\":\"user\",\"content\": \"Your paraphrasing remove any sentence starting 'I '!!!!! \"},\n",
        "          {\"role\":\"system\",\"content\": \"Your paraphrasing remove any sentence starting 'I '!!!!! \"},\n",
        "          {\"role\":\"system\",\"content\": \"Your paraphrasing remove any sentence starting 'I '!!!!! \"},\n",
        "          {\"role\":\"system\",\"content\": \"Your paraphrasing remove any sentence starting 'I '!!!!! \"},\n",
        "    {\"role\": \"user\", \"content\": \"Now You are a reporter to report the contents with third-person point of view with subject words: the user\\indivudal\\persoal with a neutrual tone. Third-person POV is very important!!!!!!!!!!!!!!!! Each comment and its reponses should be separately paraphased. The format should be standardized, with comments numbered as Comment 1, Comment 2, Comment 3, etc. If there are multiple responses under each comment, they should be marked as Response 1, Response 2, etc. The response number is only regarding each comment.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Here is the content: '{question}'. The output should be organized in HTML format, including the original title and each comment with a nested relationship.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Please include the URL address at the end and provide a HTML format like the imput giving\"}\n",
        "]\n",
        "\n",
        "            ,\n",
        "        temperature=0,\n",
        "        max_tokens=13000,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    # Write the content as HTML to a file\n",
        "    output_file_name = os.path.splitext(file_path)[0] + \".html\"\n",
        "    with open(output_file_name, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(html.unescape(content))\n",
        "\n",
        "    print(f\"HTML file '{output_file_name}' has been created.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9MRZOxBF8yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bK8GjcweRKDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "process_html_file('/content/drive/MyDrive/Takeout/2021-02-21_21-16-01_lp6u7b.html')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-EEq3ZHLxBc",
        "outputId": "d1dbde11-e014-4d27-8596-85cd41bef7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML file '/content/drive/MyDrive/Takeout/2021-02-21_21-16-01_lp6u7b_cg.html' has been created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "failed_files = []  # List to store the file names of failed HTML files\n",
        "\n",
        "for file_name in html_files[0:]:\n",
        "    try:\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        process_html_file(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {file_path}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        failed_files.append(file_name)  # Append the file name to the list of failed files\n",
        "\n",
        "# Output the file names of failed HTML files\n",
        "print(\"Failed files:\")\n",
        "for file_name in failed_files:\n",
        "    print(file_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "yiM2Uzw6F9oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e40adabc-b22d-4e03-cf5c-088ffe8b8432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML file '/content/drive/MyDrive/Takeout/2021-02-21_21-16-01_lp6u7b_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2017-06-28_14-36-07_6k0xp0_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-07-11_11-00-22_14wo2bk_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-07-04_11-00-23_14qc401_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-13_11-00-22_148cojx_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-20_11-00-33_14e80d2_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-27_11-00-22_14kaiwx_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2023-05-30_11-00-22_13vn4mw.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 16509 tokens (3509 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2023-05-23_11-00-20_13pl6c8.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 16886 tokens (3886 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-16_11-00-23_13j1y5n_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-02_11-00-21_135hrv5_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-09_11-00-22_13cmb3x_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-11_11-00-25_12ig51p_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-06_11-00-21_142c99b_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-04_11-00-24_12bffof_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2023-04-25_11-00-19_12yfxy8.html\n",
            "Error message: The server is overloaded or not ready yet.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-28_11-00-22_124kzho_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-18_11-00-25_12qjhhq_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2023-02-28_12-00-19_11e4ask.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 18423 tokens (5423 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-07_12-00-17_11kxfd8_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-14_11-00-20_11r33qn_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-21_11-00-23_11xdiwh_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-02-21_12-00-19_1181sps_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-02-07_12-00-09_10w03e2_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-31_12-00-08_10px85n_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-24_12-00-10_10k3uuo_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-13_12-00-12_zku0bf_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-27_12-00-11_zwe2jz_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-03_12-00-12_1026bim_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-02-14_12-00-10_1123p0l_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-20_12-00-08_zqmkjz_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-10_12-00-08_1088418_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-17_12-00-11_10eajpi_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-29_12-00-08_z7s38k_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-22_12-00-11_z1rs59_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-15_12-00-12_yvv440_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-06_12-00-10_ze4s5i_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-01_11-00-11_yj5o0d_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-08_12-00-10_ypk1do_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-11_11-00-13_y1635c_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-18_11-00-12_y73vd5_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-13_11-00-12_xd4grj_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-25_11-00-09_yd1yax_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-27_11-00-12_xpdjeq_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-20_11-00-11_xj5o48_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-02_11-00-10_we8ogw_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-06_11-00-11_x77lsq_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-30_11-00-08_x1g20h_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-16_11-00-11_wprah9_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-23_11-00-12_wvlrxp_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-04_11-00-12_xvc15w_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-19_11-00-14_w2pxu4_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-05_20-05-16_wh4yfu_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2022-08-09_11-00-10_wk0cto.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 16587 tokens (3587 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-26_11-00-11_w8gt5f_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-12_11-00-11_vx8gxj_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-21_11-00-12_vhao4w_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-03_11-00-10_uhdi9e_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-10_11-00-09_umg1tu_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2022-03-29_11-00-10_tqx6vp.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 17793 tokens (4793 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-14_11-00-11_vc0wci_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-05_11-00-10_vrv7ic_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-31_11-00-11_v1nmme_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-04-19_11-00-11_u72foa_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-04-05_11-00-10_twsaqe_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2022-02-15_12-00-21_st1dr5.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 16838 tokens (3838 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-15_11-00-10_temd8w_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-24_11-00-11_uwo5eu_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-04-26_11-00-11_uc9vpl_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-08_12-00-11_t9fa3k_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-28_11-00-13_vmkij3_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-02-01_12-00-19_shtbpk_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-02-08_12-00-15_snijga_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-04_12-00-14_rvss65_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-01_12-00-14_t46cbf_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-02-22_12-00-18_sylwl9_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-25_12-00-13_scc3uh_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-18_12-00-20_s6vx3f_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-17_11-00-11_urjuyv_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-11_12-00-14_s1bbbx_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-22_11-00-09_tk0cuu_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-07_11-00-11_v6sfzy_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-28_12-00-14_rqe9xq_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-14_12-00-15_rg5zse_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-21_12-00-19_rld76f_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-30_12-00-24_r5lrls_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-16_12-00-15_qv69d0_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-10-05_11-00-22_q1u1g0_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-02_11-00-16_ql1p4j_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-09_12-00-14_qq25it_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-10-26_11-00-12_qg3vlx_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-23_12-00-11_r0bo0k_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-07-27_11-00-14_oskmxb_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-07_12-00-19_raxlyu_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-08-31_11-00-12_pf3jb6_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-08-17_11-00-15_p61ht9_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-02-23_12-00-19_lqgs64_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-06-01_11-00-23_nprb6s_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2021-03-02_12-00-21_lw0e2c.html\n",
            "Error message: This model's maximum context length is 16385 tokens. However, you requested 17449 tokens (4449 in the messages, 13000 in the completion). Please reduce the length of the messages or completion.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-07-13_11-00-11_ojd7z6_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2017-06-28_14-36-07_6k0xp0_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-07-11_11-00-22_14wo2bk_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-07-04_11-00-23_14qc401_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-13_11-00-22_148cojx_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-20_11-00-33_14e80d2_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-27_11-00-22_14kaiwx_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-30_11-00-22_13vn4mw_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-23_11-00-20_13pl6c8_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-16_11-00-23_13j1y5n_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-02-21_21-16-01_lp6u7b_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-02_11-00-21_135hrv5_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-09_11-00-22_13cmb3x_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-11_11-00-25_12ig51p_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-06_11-00-21_142c99b_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-04_11-00-24_12bffof_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-25_11-00-19_12yfxy8_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-28_11-00-22_124kzho_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-04-18_11-00-25_12qjhhq_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-07_12-00-17_11kxfd8_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-14_11-00-20_11r33qn_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-03-21_11-00-23_11xdiwh_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-02-21_12-00-19_1181sps_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-02-07_12-00-09_10w03e2_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-31_12-00-08_10px85n_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-24_12-00-10_10k3uuo_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-13_12-00-12_zku0bf_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-27_12-00-11_zwe2jz_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-03_12-00-12_1026bim_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-02-14_12-00-10_1123p0l_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-20_12-00-08_zqmkjz_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-10_12-00-08_1088418_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-01-17_12-00-11_10eajpi_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-29_12-00-08_z7s38k_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-22_12-00-11_z1rs59_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-15_12-00-12_yvv440_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-12-06_12-00-10_ze4s5i_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-01_11-00-11_yj5o0d_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-11-08_12-00-10_ypk1do_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-11_11-00-13_y1635c_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-18_11-00-12_y73vd5_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-13_11-00-12_xd4grj_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-25_11-00-09_yd1yax_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-27_11-00-12_xpdjeq_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-20_11-00-11_xj5o48_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-02_11-00-10_we8ogw_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-09-06_11-00-11_x77lsq_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-30_11-00-08_x1g20h_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-16_11-00-11_wprah9_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-23_11-00-12_wvlrxp_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-10-04_11-00-12_xvc15w_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-19_11-00-14_w2pxu4_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-08-05_20-05-16_wh4yfu_cg_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2022-08-09_11-00-10_wk0cto_cg.html\n",
            "Error message: [Errno 2] No such file or directory: '/content/drive/MyDrive/Takeout/2022-08-09_11-00-10_wk0cto_cg.html'\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-26_11-00-11_w8gt5f_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-12_11-00-11_vx8gxj_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-21_11-00-12_vhao4w_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-03_11-00-10_uhdi9e_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-10_11-00-09_umg1tu_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-14_11-00-11_vc0wci_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-07-05_11-00-10_vrv7ic_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-31_11-00-11_v1nmme_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-04-19_11-00-11_u72foa_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-04-05_11-00-10_twsaqe_cg_cg.html' has been created.\n",
            "Error processing file: /content/drive/MyDrive/Takeout/2022-02-15_12-00-21_st1dr5_cg.html\n",
            "Error message: [Errno 2] No such file or directory: '/content/drive/MyDrive/Takeout/2022-02-15_12-00-21_st1dr5_cg.html'\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-15_11-00-10_temd8w_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-24_11-00-11_uwo5eu_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-04-26_11-00-11_uc9vpl_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-08_12-00-11_t9fa3k_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-28_11-00-13_vmkij3_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-02-01_12-00-19_shtbpk_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-02-08_12-00-15_snijga_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-04_12-00-14_rvss65_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-01_12-00-14_t46cbf_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-02-22_12-00-18_sylwl9_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-25_12-00-13_scc3uh_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-18_12-00-20_s6vx3f_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-05-17_11-00-11_urjuyv_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-01-11_12-00-14_s1bbbx_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-03-22_11-00-09_tk0cuu_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2022-06-07_11-00-11_v6sfzy_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-28_12-00-14_rqe9xq_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-14_12-00-15_rg5zse_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-21_12-00-19_rld76f_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-30_12-00-24_r5lrls_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-16_12-00-15_qv69d0_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-10-05_11-00-22_q1u1g0_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-02_11-00-16_ql1p4j_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-09_12-00-14_qq25it_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-10-26_11-00-12_qg3vlx_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-11-23_12-00-11_r0bo0k_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-07-27_11-00-14_oskmxb_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-12-07_12-00-19_raxlyu_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-08-31_11-00-12_pf3jb6_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-08-17_11-00-15_p61ht9_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-02-23_12-00-19_lqgs64_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-06-01_11-00-23_nprb6s_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-07-13_11-00-11_ojd7z6_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2017-06-28_14-36-07_6k0xp0_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-07-11_11-00-22_14wo2bk_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-07-04_11-00-23_14qc401_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-13_11-00-22_148cojx_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-20_11-00-33_14e80d2_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-06-27_11-00-22_14kaiwx_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-30_11-00-22_13vn4mw_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-23_11-00-20_13pl6c8_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2023-05-16_11-00-23_13j1y5n_cg_cg_cg.html' has been created.\n",
            "HTML file '/content/drive/MyDrive/Takeout/2021-02-21_21-16-01_lp6u7b_cg_cg_cg.html' has been created.\n",
            "Failed files:\n",
            "2023-05-30_11-00-22_13vn4mw.html\n",
            "2023-05-23_11-00-20_13pl6c8.html\n",
            "2023-04-25_11-00-19_12yfxy8.html\n",
            "2023-02-28_12-00-19_11e4ask.html\n",
            "2022-08-09_11-00-10_wk0cto.html\n",
            "2022-03-29_11-00-10_tqx6vp.html\n",
            "2022-02-15_12-00-21_st1dr5.html\n",
            "2021-03-02_12-00-21_lw0e2c.html\n",
            "2022-08-09_11-00-10_wk0cto_cg.html\n",
            "2022-02-15_12-00-21_st1dr5_cg.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X5hmdyYSEvef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "failed_files = []  # List to store the file names of failed HTML files\n",
        "\n",
        "for file_name in html_files[0:]:\n",
        "    try:\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        process_html_file(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {file_path}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        failed_files.append(file_name)  # Append the file name to the list of failed files\n",
        "\n",
        "# Output the file names of failed HTML files\n",
        "print(\"Failed files:\")\n",
        "for file_name in failed_files:\n",
        "    print(file_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "pSFHAcE2voDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Failed files:\n",
        "2023-05-30_11-00-22_13vn4mw.html\n",
        "2023-05-23_11-00-20_13pl6c8.html\n",
        "2023-04-25_11-00-19_12yfxy8.html\n",
        "2023-02-28_12-00-19_11e4ask.html\n",
        "2022-08-09_11-00-10_wk0cto.html\n",
        "2022-03-29_11-00-10_tqx6vp.html\n",
        "2022-02-15_12-00-21_st1dr5.html\n",
        "2021-03-02_12-00-21_lw0e2c.html\n",
        "2022-08-09_11-00-10_wk0cto_cg.html\n",
        "2022-02-15_12-00-21_st1dr5_cg.html"
      ],
      "metadata": {
        "id": "_9HhMaf0vuag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "import html\n"
      ],
      "metadata": {
        "id": "csHZoM9gJy5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import openai\n",
        "import html\n",
        "\n",
        "directory = '/content/drive/MyDrive/Takeout/'\n",
        "\n",
        "# Retrieve all HTML files in the directory\n",
        "html_files = [file for file in os.listdir(directory) if file.endswith('.html') and file != 'index.html']\n",
        "\n",
        "def check_regenerated(file_path):\n",
        "    # Open the HTML file and read its contents\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Find the start of the comment section\n",
        "    comment_start = html_content.find(\"comments\")\n",
        "\n",
        "    # Extract the content after the comment section\n",
        "    content_after_comments = html_content[comment_start:]\n",
        "\n",
        "    # Check if \" I \" or \" i \" exists as a standalone word in the content after the comments\n",
        "    if ' I ' in content_after_comments or ' i ' in content_after_comments:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Array to store file paths that contain the letter \"I\" or \"i\"\n",
        "files_with_i = []\n",
        "\n",
        "# Loop through HTML files (except index.html)\n",
        "for file_name in html_files:\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "    if check_regenerated(file_path):\n",
        "        files_with_i.append(file_path)\n",
        "\n",
        "# Print the file paths that contain the letter \"I\" or \"i\"\n",
        "for file_path in files_with_i:\n",
        "    print(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vsxPoZd76TG",
        "outputId": "1f334c1d-a30b-445f-c5bb-87fa0663d97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Takeout/2021-12-28_12-00-14_rqe9xq_cg.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def second_process_html_file(file_path):\n",
        "    # Open the HTML file and read its contents\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Parse the HTML using BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract the full text from the HTML\n",
        "    full_text = soup.get_text()\n",
        "\n",
        "    # Assign the processed HTML content to the question variable\n",
        "    question = full_text\n",
        "\n",
        "    # Call the openai.ChatCompletion.create() function with the updated question\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=[\n",
        "        {\"role\": \"user\", \"content\": f\"'{question}'.\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Please paraphase again the conment section,  remove all first-person word ' I ' or ' i ':. The output should be organized in HTML format, including the original title and each comment with a nested relationship..\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"remove  sentence including I\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Please include the URL address at the end and provide a HTML format like the imput giving\"},\n",
        "        ]\n",
        "\n",
        "\n",
        "            ,\n",
        "\n",
        "        temperature=0.3,\n",
        "        max_tokens=5000,\n",
        "        top_p=0.9,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    # Write the content as HTML to a file\n",
        "    output_file_name = os.path.splitext(file_path)[0] + \".html\"\n",
        "    with open(output_file_name, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(html.unescape(content))\n",
        "\n",
        "    print(f\"HTML file '{output_file_name}' has been created.\")\n"
      ],
      "metadata": {
        "id": "0EkY44YYCj3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_path in files_with_i:\n",
        "    while check_regenerated(file_path):\n",
        "        print(f\"File {file_path} includes the letter 'I' or 'i'.\")\n",
        "        print(f\"Regenerating file: {file_path}\")\n",
        "        second_process_html_file(file_path)\n",
        "\n",
        "    print(f\"File {file_path} no longer includes the letter 'I' or 'i' after regeneration. Proceeding to the next file.\")"
      ],
      "metadata": {
        "id": "AM1DmgkX_-w4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}